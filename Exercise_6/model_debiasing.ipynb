{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folktables.acs import adult_filter\n",
    "from folktables import ACSDataSource, BasicProblem, generate_categories\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and Preprocess the data\n",
    "We are going to work with the [Folktables](https://github.com/socialfoundations/folktables#quick-start-examples) dataset (*you have already worked with it*). I have chosen some variables for you, but you can add more (*if you like to*) - here is the [full list](https://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMS_Data_Dictionary_2021.pdf) of variables (some of them do not exist in `ACSDataSource`). \n",
    "\n",
    "Today we are going to debias a regression model using the `SEX` variable. Your model should predict the *Total person's income*  (I've digitized  it in  `target_transform=lambda x: x > 25000`, you can choose another threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')\n",
    "acs_data = data_source.get_data(states=[\"CA\"], download=True)\n",
    "\n",
    "ACSIncomeNew = BasicProblem(\n",
    "    features=[\n",
    "        'AGEP', # include AGE\n",
    "        'COW', # include class of worker\n",
    "        'SCHL', # include school education\n",
    "        'WKHP', # include reported working hours\n",
    "        'SEX', # include sex\n",
    "        'MAR', # martial status\n",
    "        # some random, possibly noisy\n",
    "        'PWGTP', # person weight\n",
    "        'JWMNP', # travel time to work\n",
    "        'INTP', # interest, devidents and etc\n",
    "    ],\n",
    "    target='PINCP',\n",
    "    target_transform=lambda x: x > 25000,    \n",
    "    group='SEX',\n",
    "    preprocess=adult_filter,\n",
    "    postprocess=lambda x: np.nan_to_num(x, -1),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a small snippet to get the names of the categorical variables - I convert categoricals into one-hot encoded (*you don't have to, depending on what assumptions you use about the data*). **Don't forget to normalise the continious features (if you plan to use Cross-Validation features should be normalized per fold, aka not in the global table).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition_df = data_source.get_definitions(download=True)\n",
    "categories = generate_categories(features=ACSIncomeNew.features, definition_df=definition_df)\n",
    "# here I convert categoricals into one-hot encoded (you don't have to, depending on what assumptions you use about the data)\n",
    "features, labels, groups = ACSIncomeNew.df_to_pandas(acs_data, categories=categories, dummies=True)\n",
    "########### Normalize continious features\n",
    "## YOUR CODE (if relevant)\n",
    "###########\n",
    "features.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting data into train-test. **Again, if you plan to use Cross-Validation then you should normalise features only inside of a fold**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "    features.values, labels.values.reshape(-1), groups, test_size=0.2, random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Regression model (without Fairness constraints)\n",
    "Let's first train a simple **Logistic Regression**. \n",
    "1. Use L2 penalty to train the model (you should find the optimal value for the regularizer)\n",
    "2. Calculate the total performance metric\n",
    "3. Calculate and compare the perforformanc metric for each `SEX` group (use your favourite metric introduced during the course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "### YOUR CODE HERE\n",
    "##########"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Constrained Regression Model \n",
    "Now let's try to include the [Fairness Constraint](https://arxiv.org/abs/1706.02409)! You'll have to implement couple of things from scratch (as it is tricky to add a custom constraint function in `sklearn`.  To optimise the cost function let's use `scipy.optimize.fmin_tnc`. To calculate gradient you can use `fprime` attribute):\n",
    "1. Logistic Regression\n",
    "2. L2 penalisation\n",
    "3. **Group** Fairness Constrained\n",
    "\n",
    "When you are finished with implementation - you should evaluate performance on multiple choices of fairness weight, $\\lambda$.\n",
    "\n",
    "**Note**: You should include $x_0 = 1$ in your data, for each observation (when it comes to the manual implementation of logistic regression) to include bias (i.e. weight $\\beta_0$)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detailed breakdown\n",
    "The group constraint constraint looks like this:\n",
    "\n",
    "$$ \n",
    "f(\\beta,S) = \\left( \\frac{1}{n_1 n_2} \\sum\\limits{(x_i,y_i)\\in S_1 \\\\ (x_i,y_i)\\in S_2} d(y_i,y_j) (\\beta^T  \\textbf{x}_i - \\beta^T \\textbf{x}_j)  \\right)^2 \n",
    "$$\n",
    "\n",
    "\n",
    "For the constrained optimization we have to solve a problem on the form:\n",
    "\n",
    "$$ \\min_\\beta \\left( \\ell (\\beta,S) + \\lambda f(\\beta,S)  +\\gamma \\Vert \\beta \\Vert_2 \\right) $$ \n",
    "\n",
    "where $\\ell$ is some loss function, $f$ is the constraint function, and the $\\gamma \\Vert \\beta \\Vert_2 $ is L2 regularization (we use it to avoid overfitting).\n",
    "(Basically we are minimizing the Lagrangian $\\mathscr{L} = \\ell (\\beta,S) + \\lambda f(\\beta,S)  +\\gamma \\Vert \\textbf{x} \\Vert_2$ with respect to $\\beta$ - in ML literature $\\mathscr{L}$ is often denoted as J)\n",
    "\n",
    "Because we are doing classification we are going to use logistic regression. The log loss function is:\n",
    "$$\n",
    "\\ell = \\frac{1}{m}\\sum_i^m\\left[ -y_i \\log(g(x_i)) - (1-y_i)\\log(1-g(x_i)) \\right], \\text{where } g(x_i) = \\frac{1}{1+\\exp(-\\beta_i x_i)}\n",
    "$$\n",
    "\n",
    "For the distance function we follow the approach from Berk et al. (2017) and set:\n",
    "$$d(y_i,y_j) = \\begin{cases}\n",
    "            1, &         \\text{if } y_i=y_j,\\\\\n",
    "            0, &         \\text{if } y_i\\neq y_j.\n",
    "    \\end{cases}$$\n",
    "    \n",
    "To minimize the total loss function we also need to estimate the gradient of $\\mathscr{L}$ with respect to $\\beta$. Here to update the $\\beta$ values we are just going the gradient's without the fairness constraing - this will make our lives considerably easier. The j'th element of the gradiend is defined as follows:\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\beta_j} \\approx \\frac{1}{m}\\left( \\sum_i  (g(x_i) - y_i) x[j] \\right)+ 2\\gamma \\beta_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"This is logistic regression\"\"\"\n",
    "    NotImplemented\n",
    "def compute_gradient(beta,X,y, groups, lambda_,gamma_):\n",
    "    \"\"\"Calculate the gradient - used for finding the best beta values\"\"\"\n",
    "\n",
    "    NotImplemented\n",
    "def compute_cost(beta,X,y, groups, lambda_,gamma_):\n",
    "    \"\"\"Computes cost function with constraints\"\"\"\n",
    "    NotImplemented\n",
    "\n",
    "\n",
    "########## This is the optimisation function\n",
    "#result = opt.fmin_tnc(func=compute_cost, x0=beta, fprime = compute_gradient, maxfun = 500,\n",
    "#                          args = (X_train,Y_train,lambda_,gamma_), xtol=1e-7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
